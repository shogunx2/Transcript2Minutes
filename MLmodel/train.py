# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKxpCv3FN33Ekaxe56xDzUZOL4cRAFG2
"""

from huggingface_hub import login
login()

!pip install -q transformers datasets torch

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import load_dataset
import torch

print("="*60)
print("FLAN-T5 Fine-Tuning for Meeting Minutes Generation")
print("="*60)

# Check GPU availability
print(f"\nGPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")

# Load AMI meeting dataset
print("\nLoading AMI meeting dataset...")
dataset = load_dataset("knkarthick/AMI")

print(f"Training samples: {len(dataset['train'])}")
print(f"Validation samples: {len(dataset['validation'])}")
print(f"Test samples: {len(dataset['test'])}")

# Initialize FLAN-T5 model and tokenizer
model_name = "google/flan-t5-base"
print(f"\nLoading model: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Preprocessing function
def preprocess(examples):
    inputs = [f"Generate meeting minutes from this transcript: {doc}" for doc in examples["dialogue"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    labels = tokenizer(examples["summary"], max_length=150, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

# Preprocess dataset
print("\nPreprocessing dataset...")
processed_train = dataset["train"].map(preprocess, batched=True, remove_columns=dataset["train"].column_names)
processed_val = dataset["validation"].map(preprocess, batched=True, remove_columns=dataset["validation"].column_names)
processed_test = dataset["test"].map(preprocess, batched=True, remove_columns=dataset["test"].column_names)

print(f"Processed training samples: {len(processed_train)}")
print(f"Processed validation samples: {len(processed_val)}")

# Define training arguments
training_args = TrainingArguments(
    output_dir="./flan_t5_meeting_minutes",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    learning_rate=2e-4,
    weight_decay=0.01,
    logging_steps=50,
    save_steps=200,
    eval_steps=200,
    evaluation_strategy="steps",  # ADD THIS LINE - CRITICAL FIX
    save_strategy="steps",        # ADD THIS LINE
    save_total_limit=2,
    warmup_steps=200,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    dataloader_num_workers=4,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_train,
    eval_dataset=processed_val,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
)

# Train the model
print("\n" + "="*60)
print("Starting Training...")
print("="*60)
trainer.train()

# Save the fine-tuned model
print("\n" + "="*60)
print("Training Complete!")
print("="*60)
print("Saving model and tokenizer...")
model.save_pretrained("./flan_t5_meeting_minutes")
tokenizer.save_pretrained("./flan_t5_meeting_minutes")

# Evaluate on test set
print("\nEvaluating on test set...")
test_results = trainer.evaluate(eval_dataset=processed_test)
print(f"Test Loss: {test_results['eval_loss']:.4f}")

print("\nModel saved in ./flan_t5_meeting_minutes")
print("Ready for download and integration!")

!zip -r flan_t5_meeting_minutes.zip flan_t5_meeting_minutes

!cp flan_t5_meeting_minutes.zip /content/drive/MyDrive/