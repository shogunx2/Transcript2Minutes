# Transcript2Minutes - Meeting Transcript Summarizer

A full-stack application that converts meeting transcripts into concise summaries using a T5 model fine-tuned on the AMI meeting corpus.

## ğŸ¯ Project Structure

```
Transcript2Minutes/
â”œâ”€â”€ frontend/              # Vue.js modern UI (dark theme)
â”œâ”€â”€ backend/               # Flask REST API
â”œâ”€â”€ MLservice/             # T5 model inference service
â”œâ”€â”€ t5_ami_meeting/        # Fine-tuned T5 model (trained on AMI corpus)
â”œâ”€â”€ Untitled2.ipynb        # Training notebook (Colab reference)
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 16+
- 8GB+ VRAM (for GPU inference, optional)

### 1. Frontend Setup
```bash
cd frontend
npm install
npm run dev
```
Runs on `http://localhost:5173`

### 2. MLservice Setup (Model Inference)
```bash
cd MLservice
pip install -r requirements.txt
python app.py
```
Runs on `http://localhost:5001`

### 3. Backend Setup
```bash
cd backend
pip install -r requirements.txt
python app.py
```
Runs on `http://localhost:5000`

## âœ¨ Features

- **Modern Dark UI**: Clean two-panel interface (input/output)
- **Multiple File Formats**: Supports .txt, .pdf, .doc, .docx
- **Real-time Summarization**: T5 model fine-tuned on AMI meeting corpus
- **Word Counter**: Live word count with 4000 word limit
- **File Upload**: Drag-and-drop or click to upload
- **Fast Inference**: GPU-optimized with PyTorch

## ğŸ§  Model Details

- **Architecture**: T5-base (220M parameters)
- **Training Data**: AMI meeting corpus (~14K conversations)
- **Task**: "summarize: {dialogue}" â†’ {summary}
- **Training Config**:
  - Epochs: 5
  - Batch size: 16
  - Learning rate: 5e-5
  - Max input: 512 tokens
  - Max output: 150 tokens
  - Mixed precision: FP16

## ğŸ“Š Architecture

```
Vue.js Frontend
      â†“
Flask Backend API (/api/summarize)
      â†“
MLservice (T5 Inference)
      â†“
Formatted Summary
      â†“
JSON Response â†’ Frontend Display
```

## ğŸ® How to Use

1. **Paste Text**: Copy-paste meeting transcript in left panel
2. **Upload File**: Click "Attach File" to upload .txt, .pdf, etc.
3. **Auto-Generate**: Summary appears automatically in right panel
4. **Monitor**: Word counter shows text length (limit: 4000 words)

## ğŸ“ Configuration

### Model Path
- Default: `../t5_ami_meeting` (relative to MLservice)
- Fine-tuned weights loaded automatically on startup

### Inference Parameters
- Max tokens: 150
- Beam search: 4 beams
- Early stopping: Enabled
- Temperature: 0.7

### Word Limits
- Input max: 4000 words (configurable in frontend)
- Output target: 10-20% of input

## ğŸ”§ Environment Variables

Create `.env` file in root:
```
FLASK_ENV=development
MODEL_PATH=../t5_ami_meeting
MAX_INPUT_LENGTH=4000
```

## ğŸ“¦ Dependencies

**Frontend**: Vue 3, Axios, PDF.js
**Backend**: Flask, Flask-CORS, Requests
**MLservice**: PyTorch, Transformers, Hugging Face
**Training**: See `Untitled2.ipynb` for full setup

## ğŸ”„ Workflow

```python
# Training (Colab - see Untitled2.ipynb)
Dataset: AMI meeting corpus
â†“
Fine-tune T5-base for 5 epochs
â†“
Save model to t5_ami_meeting/

# Inference
Frontend input â†’ Backend API â†’ MLservice model
â†“
Generate summary using beam search
â†“
Return formatted output
```

## ğŸš€ Deployment

### Local Development
```bash
# Terminal 1: Frontend
cd frontend && npm run dev

# Terminal 2: MLservice
cd MLservice && python app.py

# Terminal 3: Backend
cd backend && python app.py
```

### Production
- Build frontend: `npm run build`
- Use production WSGI server (Gunicorn)
- Add authentication for API endpoints
- Configure HTTPS

## ğŸ§ª Testing

Test the API directly:
```bash
curl -X POST http://localhost:5001/summarize \
  -H "Content-Type: application/json" \
  -d '{"transcript":"Person A: Let'\''s discuss Q4 goals. Person B: Sure, we should focus on customer retention."}'
```

## ğŸ“ˆ Performance

- **Inference Speed**: ~2-5 seconds per 1000-word transcript (GPU)
- **Memory**: ~2GB (model loaded) + ~1GB (inference)
- **Accuracy**: ROUGE-1: ~0.38 (AMI corpus baseline)

## ğŸ›  Troubleshooting

### Model not loading
- Check path: `ls t5_ami_meeting/`
- Verify files: `pytorch_model.bin`, `config.json`, `spiece.model`

### Out of memory
- Use CPU: Set `CUDA_VISIBLE_DEVICES=""`
- Reduce input length
- Use t5-small model

### API errors
- Check CORS enabled in backend
- Verify port availability
- Check error logs in terminal

## ğŸ“š References

- **T5 Paper**: [Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683)
- **AMI Corpus**: [Meeting Corpus Dataset](https://groups.inf.ed.ac.uk/ami/corpus/)
- **Hugging Face Transformers**: [Documentation](https://huggingface.co/transformers/)

## ğŸ“ Training Notebook

See `Untitled2.ipynb` for the complete training pipeline using Google Colab:
- Dataset loading
- Model configuration
- Training loop
- Checkpoint management
- Metrics logging

## ğŸ¨ UI Features

- Dark theme with teal accents
- Real-time word counting
- Error message display
- Responsive design (mobile-friendly)
- Smooth animations
- Custom scrollbars

## ğŸ” Notes

- Model is automatically loaded on MLservice startup
- Fine-tuned weights take precedence over base model
- All processing done locally (no external API calls)
- Input text limited to 4000 words (configurable)

---

**Last Updated**: October 2025
**Model**: T5-base (fine-tuned on AMI corpus)
**Status**: Production Ready âœ…

- **Frontend Dev Server**: Vite with hot reload
- **Backend Dev Mode**: Flask debug mode
- **Model Training**: Distributed training support with Hugging Face Transformers

## Next Steps

1. Run MLmodel training pipeline (`MLmodel/train.py`)
2. Start MLservice with trained model
3. Launch Backend Flask API
4. Open Frontend interface
5. Test with sample transcripts

## License

MIT

## Notes

- All processed files are discarded immediately (no storage)
- Summary format is easily customizable in `MLservice/utils/formatter.py`
- Model and dataset configurations in `MLmodel/config.yaml`
